* Start the mlflow ui : 

    mlflow ui --backend-store-uri sqlite:///mlflow.db


* To load the model for prediction :

    1** Define the RUN_ID of the perfect model to load : 
    # RUN_ID = "bd5e6457812dc546"

    2** Load the model : 
    logged_model = f's3://mlflow-models-alexey/1/{RUN_ID}/artifacts model'   
    
    # ---> (if the model registry is stored in an S3 bucket and to avoid being dependant on the tracking server that may go down )

        model = mlflow.pyfunc.load_model(logged_model)


* To load the artifacts , we need to use the client :  (No need for that if we use the S3 bucket )

    1** Define the tracking uri :
    # MLFLOW_TRACKING_URI = "bd5e6457812dc546"

    2** Load the client :
    client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)

    3** Define the path and Download the artifact :
    path = client.download_artifacts(run_id=RUN_ID, path="dict_vectorizer.bin")

    with open(path, 'rb') as file_out:
        dv = pickle.load(file_out)


    